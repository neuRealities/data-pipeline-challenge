{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a28e66",
   "metadata": {},
   "source": [
    "# CT Training Data Pipeline — Validation\n",
    "## End-to-End Checks (Bronze → Silver)\n",
    "\n",
    "Purpose:\n",
    "- Confirm Bronze stream is writing to Delta on MinIO.\n",
    "- Confirm Silver batch aggregates are present and partitioned.\n",
    "- Compute core KPIs for a quick health signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b88c8",
   "metadata": {},
   "source": [
    "## 1. Setup — Spark Session\n",
    "Relies on `spark/apps/spark_config.py` shipped with this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-project-root",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def add_project_root(marker_rel=\"spark/apps/spark_config.py\", max_up=8):\n",
    "    cur = Path.cwd().resolve()\n",
    "    for _ in range(max_up):\n",
    "        if (cur / marker_rel).is_file():\n",
    "            if str(cur) not in sys.path:\n",
    "                sys.path.insert(0, str(cur))\n",
    "            print(\"Using project root:\", cur)\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(f\"Couldn't find {marker_rel} within {max_up} levels up from {Path.cwd()}\")\n",
    "\n",
    "PROJECT_ROOT = add_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-spark-local-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Spark with Delta (no spark-master)\n",
    "# Puts Delta + S3A jars on the *driver* classpath and starts Spark local[*].\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import urllib.request, os, gc\n",
    "\n",
    "# 0) Clean reset to avoid stale configs\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "SparkSession._instantiatedSession = None\n",
    "SparkContext._active_spark_context = None\n",
    "gc.collect()\n",
    "\n",
    "# 1) Find project root (matches the layout) and jars dir\n",
    "def find_project_root(marker_rel=\"spark/apps/spark_config.py\", max_up=8):\n",
    "    cur = Path.cwd().resolve()\n",
    "    for _ in range(max_up):\n",
    "        if (cur / marker_rel).is_file():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "JARS_DIR = PROJECT_ROOT / \"spark\" / \"jars\"\n",
    "JARS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2) Ensure required JARs are present (download if missing)\n",
    "SCALA_BIN = \"2.12\"\n",
    "DELTA_VER = \"3.3.0\"\n",
    "HADOOP_AWS_VER = \"3.3.4\"\n",
    "AWS_SDK_VER = \"1.12.300\"\n",
    "\n",
    "maven = \"https://repo1.maven.org/maven2\"\n",
    "jars_to_fetch = {\n",
    "    f\"delta-spark_{SCALA_BIN}-{DELTA_VER}.jar\":\n",
    "        f\"{maven}/io/delta/delta-spark_{SCALA_BIN}/{DELTA_VER}/delta-spark_{SCALA_BIN}-{DELTA_VER}.jar\",\n",
    "    f\"delta-storage-{DELTA_VER}.jar\":\n",
    "        f\"{maven}/io/delta/delta-storage/{DELTA_VER}/delta-storage-{DELTA_VER}.jar\",\n",
    "    f\"hadoop-aws-{HADOOP_AWS_VER}.jar\":\n",
    "        f\"{maven}/org/apache/hadoop/hadoop-aws/{HADOOP_AWS_VER}/hadoop-aws-{HADOOP_AWS_VER}.jar\",\n",
    "    f\"aws-java-sdk-bundle-{AWS_SDK_VER}.jar\":\n",
    "        f\"{maven}/com/amazonaws/aws-java-sdk-bundle/{AWS_SDK_VER}/aws-java-sdk-bundle-{AWS_SDK_VER}.jar\",\n",
    "}\n",
    "\n",
    "def ensure_jar(fname, url):\n",
    "    path = JARS_DIR / fname\n",
    "    if not path.exists():\n",
    "        print(f\"Downloading {fname} ...\")\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "    return str(path.resolve())\n",
    "\n",
    "# Start with required jars, then include any others already in spark/jars\n",
    "jar_paths = [ensure_jar(n, u) for n, u in jars_to_fetch.items()]\n",
    "for p in sorted(JARS_DIR.glob(\"*.jar\")):\n",
    "    sp = str(p.resolve())\n",
    "    if sp not in jar_paths:\n",
    "        jar_paths.append(sp)\n",
    "\n",
    "# Build classpaths (',' for spark.jars, os.pathsep for *extraClassPath)\n",
    "spark_jars_csv = \",\".join(jar_paths)\n",
    "driver_cp = os.pathsep.join(jar_paths)\n",
    "\n",
    "# 3) Build Spark local session with Delta extensions\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"validate-pipeline\")\n",
    "      .master(\"local[*]\")\n",
    "      .config(\"spark.jars\", spark_jars_csv)\n",
    "      .config(\"spark.driver.extraClassPath\", driver_cp)   # <-- critical for driver\n",
    "      .config(\"spark.executor.extraClassPath\", driver_cp) # safe for local[*]\n",
    "      .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "      .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "# 4) Sanity checks (must succeed)\n",
    "_ = spark._jvm.java.lang.Class.forName(\"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "print(\"Delta extension loaded\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Spark master:\", spark.sparkContext.master)\n",
    "print(\"UTC now:\", datetime.now(timezone.utc).isoformat())\n",
    "print(\"Extensions:\", spark.sparkContext.getConf().get(\"spark.sql.extensions\"))\n",
    "print(\"Using\", len(jar_paths), \"JARs from:\", JARS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebbb61-05cc-4786-9675-8ae9f3c24740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in `s3a()`\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from spark.apps.spark_config import s3a as _s3a\n",
    "    s3a = _s3a\n",
    "    print(\"Using s3a() from repo: spark/apps/spark_config.py\")\n",
    "except Exception as e:\n",
    "    print(\"Repo s3a() unavailable, using simple fallback:\", e)\n",
    "    def s3a(*parts):\n",
    "        \"\"\"\n",
    "        Minimal s3a path builder: s3a://<S3_BUCKET>/<joined parts>\n",
    "        Uses S3_BUCKET env var (defaults to 'datalake').\n",
    "        \"\"\"\n",
    "        bucket = os.getenv(\"S3_BUCKET\", \"datalake\")\n",
    "        key = \"/\".join(p.strip(\"/\") for p in parts if p)\n",
    "        return f\"s3a://{bucket}/{key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55f604-c889-46e2-b6c6-7168cbd2d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3A (MinIO) at runtime\n",
    "import os\n",
    "hc = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "endpoint = (\n",
    "    os.getenv(\"S3_ENDPOINT\")\n",
    "    or os.getenv(\"MINIO_ENDPOINT\")\n",
    "    or \"http://localhost:9000\"\n",
    ")\n",
    "access  = os.getenv(\"S3_ACCESS_KEY\") or os.getenv(\"AWS_ACCESS_KEY_ID\") or \"minioadmin\"\n",
    "secret  = os.getenv(\"S3_SECRET_KEY\") or os.getenv(\"AWS_SECRET_ACCESS_KEY\") or \"minioadmin\"\n",
    "pathsty = os.getenv(\"S3_PATH_STYLE\", \"true\")\n",
    "ssl_on  = \"false\" if endpoint.startswith(\"http://\") else \"true\"\n",
    "\n",
    "hc.set(\"fs.s3a.endpoint\", endpoint)\n",
    "hc.set(\"fs.s3a.path.style.access\", pathsty)\n",
    "hc.set(\"fs.s3a.access.key\", access)\n",
    "hc.set(\"fs.s3a.secret.key\", secret)\n",
    "hc.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "hc.set(\"fs.s3a.connection.ssl.enabled\", ssl_on)\n",
    "\n",
    "print(\"S3A endpoint      :\", endpoint)\n",
    "print(\"S3A path-style    :\", pathsty)\n",
    "print(\"S3A bucket (env)  :\", os.getenv(\"S3_BUCKET\", \"datalake\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "version-checks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick library version check\n",
    "import pyspark\n",
    "print(\"PySpark:\", pyspark.__version__)\n",
    "try:\n",
    "    import importlib.metadata as importlib_metadata\n",
    "    print(\"delta-spark:\", importlib_metadata.version(\"delta-spark\"))\n",
    "except Exception as e:\n",
    "    print(\"delta-spark version check skipped:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f144955",
   "metadata": {},
   "source": [
    "## 2. Paths and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRONZE      = s3a(\"bronze\", \"lms_events\")\n",
    "SILVER      = s3a(\"silver\", \"user_progress\")\n",
    "CHECKPOINTS = s3a(\"_checkpoints\")\n",
    "\n",
    "print(\"Bronze     :\", BRONZE)\n",
    "print(\"Silver     :\", SILVER)\n",
    "print(\"Checkpoints:\", CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b80a894",
   "metadata": {},
   "source": [
    "## 3. Load — Bronze and Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-bronze-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as F_max, countDistinct, col\n",
    "\n",
    "bronze = spark.read.format(\"delta\").load(BRONZE)\n",
    "silver = spark.read.format(\"delta\").load(SILVER)\n",
    "\n",
    "print(\"Bronze count:\", bronze.count())\n",
    "print(\"Silver count:\", silver.count())\n",
    "\n",
    "print(\"\\nBronze schema:\")\n",
    "bronze.printSchema()\n",
    "\n",
    "print(\"\\nSilver schema:\")\n",
    "silver.printSchema()\n",
    "\n",
    "latest_bronze_ts = bronze.agg(F_max(\"timestamp_iso\").alias(\"ts\")).collect()[0][\"ts\"]\n",
    "latest_silver_ts = silver.agg(F_max(\"processing_timestamp\").alias(\"ts\")).collect()[0][\"ts\"]\n",
    "print(\"\\nLatest bronze timestamp_iso:\", latest_bronze_ts)\n",
    "print(\"Latest silver processing_timestamp:\", latest_silver_ts)\n",
    "\n",
    "unique_bronze_events = bronze.select(\"event_type\").distinct().count()\n",
    "print(\"Unique bronze event types:\", unique_bronze_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b264c",
   "metadata": {},
   "source": [
    "## 4. Sanity — Required Event Types Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sanity-required-events",
   "metadata": {},
   "outputs": [],
   "source": [
    "required = {\n",
    "    \"course_enrolled\", \"video_started\", \"video_completed\",\n",
    "    \"quiz_attempted\", \"quiz_submitted\", \"article_viewed\"\n",
    "}\n",
    "present = set(r[0] for r in bronze.select(\"event_type\").distinct().collect())\n",
    "missing = sorted(list(required - present))\n",
    "print(\"Missing required event types:\", missing)\n",
    "assert not missing, f\"Missing event types: {missing}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51812c9c",
   "metadata": {},
   "source": [
    "## 5. KPIs — Aggregates from Silver\n",
    "Daily rollup per user_id. Metrics match the Silver transform contract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kpis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg as F_avg, round as F_round, desc\n",
    "\n",
    "kpi = silver.agg(\n",
    "    F_round(F_avg(\"quiz_completion_rate_percent\"), 1).alias(\"avg_quiz_completion_%\"),\n",
    "    F_round(F_avg(\"video_completion_rate_percent\"), 1).alias(\"avg_video_completion_%\"),\n",
    "    F_round(F_avg(\"completion_score\"), 1).alias(\"avg_completion_score\")\n",
    ")\n",
    "kpi.show(truncate=False)\n",
    "\n",
    "print(\"Top courses by enrollments (from Bronze):\")\n",
    "(\n",
    "    bronze.where(col(\"event_type\") == \"course_enrolled\")\n",
    "          .groupBy(\"course_id\")\n",
    "          .count()\n",
    "          .orderBy(desc(\"count\"))\n",
    "          .show(10, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8f422",
   "metadata": {},
   "source": [
    "## 6. Engagement Distribution\n",
    "Counts and percentages by engagement_level (Low/Medium/High)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engagement-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as F_sum, round as F_round, col, desc\n",
    "\n",
    "dist = silver.groupBy(\"engagement_level\").count()\n",
    "total = silver.count()\n",
    "pct = dist.withColumn(\"percentage\", F_round((col(\"count\")/total)*100, 1))\n",
    "pct.orderBy(desc(\"count\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54dcd89",
   "metadata": {},
   "source": [
    "## 7. Health — Minimal SLOs\n",
    "- Bronze has data and all required event types.\n",
    "- Silver has data for at least one partition date.\n",
    "- Active users and courses are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "health-slo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as F_max\n",
    "\n",
    "bronze_cnt = bronze.count()\n",
    "silver_cnt = silver.count()\n",
    "event_types_ok = (unique_bronze_events == 6)\n",
    "active_users = bronze.select(\"user_id\").distinct().count()\n",
    "active_courses = bronze.select(\"course_id\").distinct().count()\n",
    "\n",
    "print(\"Bronze records:\", bronze_cnt)\n",
    "print(\"Silver records:\", silver_cnt)\n",
    "print(\"Event types OK:\", event_types_ok)\n",
    "print(\"Active users:\", active_users)\n",
    "print(\"Active courses:\", active_courses)\n",
    "\n",
    "status = all([\n",
    "    bronze_cnt > 0,\n",
    "    silver_cnt > 0,\n",
    "    event_types_ok,\n",
    "    active_users > 0,\n",
    "    active_courses > 0,\n",
    "])\n",
    "print(\"\\nPIPELINE STATUS:\", \"HEALTHY\" if status else \"NEEDS_ATTENTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c595da95",
   "metadata": {},
   "source": [
    "## 8. Chart — Events by Type (from Bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chart-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pdf = (bronze.groupBy(\"event_type\").count().orderBy(\"event_type\")).toPandas()\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.bar(pdf[\"event_type\"], pdf[\"count\"])  # default colors\n",
    "    plt.title(\"Events by Type (Bronze)\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Chart skipped:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321303f",
   "metadata": {},
   "source": [
    "## 9. Business KPIs\n",
    "Concrete questions derived from Bronze events to complement the validation checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kpi-param",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters — set once for this section\n",
    "COURSE_ID = \"ct_anatomy_fundamentals\"  # change as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44609f",
   "metadata": {},
   "source": [
    "### 9.1 Average quiz score per course (Bronze)\n",
    "Uses `quiz_submitted` events' `payload.score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "avg-quiz-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg as F_avg, round as F_round, col, desc\n",
    "\n",
    "avg_quiz_score_per_course = (\n",
    "    spark.read.format(\"delta\").load(BRONZE)\n",
    "    .where(col(\"event_type\") == \"quiz_submitted\")\n",
    "    .select(\"course_id\", col(\"payload.score\").alias(\"score\"))\n",
    "    .groupBy(\"course_id\")\n",
    "    .agg(F_round(F_avg(\"score\"), 2).alias(\"avg_quiz_score\"))\n",
    "    .orderBy(desc(\"avg_quiz_score\"))\n",
    ")\n",
    "\n",
    "avg_quiz_score_per_course.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5286a3fa",
   "metadata": {},
   "source": [
    "### 9.2 Most-watched video (Bronze)\n",
    "Ranks by total watch time across all `video_completed` events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "most-watched",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as F_sum, desc, col\n",
    "\n",
    "most_watched_videos = (\n",
    "    spark.read.format(\"delta\").load(BRONZE)\n",
    "    .where(col(\"event_type\") == \"video_completed\")\n",
    "    .select(col(\"payload.video_id\").alias(\"video_id\"),\n",
    "            col(\"payload.watch_seconds\").alias(\"watch_seconds\"))\n",
    "    .groupBy(\"video_id\")\n",
    "    .agg(F_sum(\"watch_seconds\").alias(\"total_watch_seconds\"))\n",
    "    .orderBy(desc(\"total_watch_seconds\"))\n",
    ")\n",
    "\n",
    "most_watched_videos.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a2609",
   "metadata": {},
   "source": [
    "### 9.3 Users who completed a specific course yesterday (UTC)\n",
    "Two definitions are computed for clarity:\n",
    "- **Strict**: user submitted a quiz and `passed = true` for that course yesterday.\n",
    "- **Loose**: user submitted any quiz for that course yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-users",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col, lit\n",
    "\n",
    "bronze_df = spark.read.format(\"delta\").load(BRONZE)\n",
    "YESTERDAY = spark.sql(\"select date_sub(current_date(), 1) as d\").collect()[0][\"d\"]\n",
    "\n",
    "completed_users_strict = (\n",
    "    bronze_df\n",
    "    .where((col(\"event_type\") == \"quiz_submitted\") & (col(\"course_id\") == COURSE_ID))\n",
    "    .withColumn(\"event_date\", to_date(col(\"timestamp_iso\")))\n",
    "    .where(col(\"event_date\") == lit(YESTERDAY))\n",
    "    .where(col(\"payload.passed\") == True)\n",
    "    .select(\"user_id\").distinct()\n",
    ")\n",
    "\n",
    "completed_users_loose = (\n",
    "    bronze_df\n",
    "    .where((col(\"event_type\") == \"quiz_submitted\") & (col(\"course_id\") == COURSE_ID))\n",
    "    .withColumn(\"event_date\", to_date(col(\"timestamp_iso\")))\n",
    "    .where(col(\"event_date\") == lit(YESTERDAY))\n",
    "    .select(\"user_id\").distinct()\n",
    ")\n",
    "\n",
    "print(\"YESTERDAY (UTC):\", YESTERDAY)\n",
    "print(\"Course:\", COURSE_ID)\n",
    "print(\"Completed (strict, passed=True):\", completed_users_strict.count())\n",
    "print(\"Completed (loose, any quiz submitted):\", completed_users_loose.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2af8cf",
   "metadata": {},
   "source": [
    "### 9.4 Persist KPI outputs to CSV in MinIO (S3A)\n",
    "Writes partitioned CSVs with headers under `s3a://datalake/analytics/...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persist-kpis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "METRIC_DATE = YESTERDAY  # reuse yesterday from 9.3\n",
    "\n",
    "# 1) Average quiz score per course\n",
    "(\n",
    "    avg_quiz_score_per_course\n",
    "    .withColumn(\"metric_date\", lit(METRIC_DATE))\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .partitionBy(\"metric_date\")\n",
    "    .csv(s3a(\"analytics\", \"avg_quiz_score_per_course\"))\n",
    ")\n",
    "\n",
    "# 2) Most-watched videos (by total watch time)\n",
    "(\n",
    "    most_watched_videos\n",
    "    .withColumn(\"metric_date\", lit(METRIC_DATE))\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .partitionBy(\"metric_date\")\n",
    "    .csv(s3a(\"analytics\", \"most_watched_videos\"))\n",
    ")\n",
    "\n",
    "# 3) Course completion counts (strict vs loose) for the selected course\n",
    "strict_count = completed_users_strict.count()\n",
    "loose_count  = completed_users_loose.count()\n",
    "\n",
    "completion_rows = [\n",
    "    Row(definition=\"strict_passed\", course_id=COURSE_ID, users=int(strict_count)),\n",
    "    Row(definition=\"loose_any_quiz\", course_id=COURSE_ID, users=int(loose_count)),\n",
    "]\n",
    "completion_df = spark.createDataFrame(completion_rows)\n",
    "\n",
    "(\n",
    "    completion_df\n",
    "    .withColumn(\"metric_date\", lit(METRIC_DATE))\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .partitionBy(\"metric_date\", \"course_id\")\n",
    "    .csv(s3a(\"analytics\", \"course_completion\"))\n",
    ")\n",
    "\n",
    "print(\"CSV exports complete:\")\n",
    "print(\" -\", s3a(\"analytics\", \"avg_quiz_score_per_course\"))\n",
    "print(\" -\", s3a(\"analytics\", \"most_watched_videos\"))\n",
    "print(\" -\", s3a(\"analytics\", \"course_completion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59114c3",
   "metadata": {},
   "source": [
    "## 10. Teardown\n",
    "Stop the Spark session to release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "teardown",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759390c-1352-479a-aed4-0da5ef26e472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
