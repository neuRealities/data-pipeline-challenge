# ============================================================
# DATA PIPELINE CHALLENGE - DOCKER COMPOSE
# - Pinned images to avoid drift
# - Works on Mac (Apple Silicon) via amd64 emulation (see notes)
# - Airflow Spark connection created by env var (no UI clicks)
# - MinIO bucket auto-created (no UI)
# - Matches variables defined in .env.example
# ============================================================

services:
  # ----------------------------------------------------------
  # Apache ZooKeeper (Kafka metadata)
  # ----------------------------------------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0     # pin release
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"                             # client port
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD", "bash", "-lc", "echo ruok | nc -w 2 localhost 2181 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ----------------------------------------------------------
  # Apache Kafka (brokers events from producer to Spark)
  # ----------------------------------------------------------
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "29092:29092"
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:29092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ----------------------------------------------------------
  # LMS producer (Python â†’ Kafka)
  # ----------------------------------------------------------
  ct-producer:
    build:
      context: .
      dockerfile: Dockerfile.producer
    container_name: ct-producer
    profiles: ["producer"]
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: "${KAFKA_BOOTSTRAP_SERVERS}"
      KAFKA_TOPIC: "${KAFKA_TOPIC}"
      EVENTS_PER_SECOND: "${EVENTS_PER_SECOND}"

  # ----------------------------------------------------------
  # Spark master
  # ----------------------------------------------------------
  spark-master:
    image: bitnami/spark:3.5.2
    platform: linux/amd64
    hostname: spark-master
    container_name: spark-master
    environment:
      SPARK_MODE: master
      SPARK_USER: spark
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
      S3_ACCESS_KEY: "${S3_ACCESS_KEY}"
      S3_SECRET_KEY: "${S3_SECRET_KEY}"
    ports:
      - "8081:8080"
      - "7077:7077"
    volumes:
      - ./spark/apps:/opt/spark/apps
      - ./spark/data:/opt/spark/data
      - ./spark/jars:/opt/extra-jars:ro
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'echo > /dev/tcp/spark-master/7077 && echo > /dev/tcp/spark-master/8080'"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 180s

  # ----------------------------------------------------------
  # Spark worker
  # ----------------------------------------------------------
  spark-worker:
    image: bitnami/spark:3.5.2
    platform: linux/amd64
    hostname: spark-worker
    container_name: spark-worker
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: "${SPARK_MASTER_URL}"
      SPARK_WORKER_MEMORY: 6G
      SPARK_WORKER_CORES: 4
      SPARK_USER: spark
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
      S3_ACCESS_KEY: "${S3_ACCESS_KEY}"
      S3_SECRET_KEY: "${S3_SECRET_KEY}"
    volumes:
      - ./spark/apps:/opt/spark/apps
      - ./spark/data:/opt/spark/data
      - ./spark/jars:/opt/extra-jars:ro
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro

  # ----------------------------------------------------------
  # MinIO
  # ----------------------------------------------------------
  minio:
    image: minio/minio:RELEASE.2025-03-12T18-04-18Z
    hostname: minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: "${S3_ACCESS_KEY}"
      MINIO_ROOT_PASSWORD: "${S3_SECRET_KEY}"
    volumes:
      - ./minio/data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio-setup:
    image: minio/mc:RELEASE.2025-03-12T17-29-24Z
    container_name: minio-setup
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    environment:
      MC_HOST_minio: "http://${S3_ACCESS_KEY}:${S3_SECRET_KEY}@minio:9000"
    command: >
      "
      /usr/bin/mc ls minio || /usr/bin/mc alias set minio http://minio:9000 ${S3_ACCESS_KEY} ${S3_SECRET_KEY} ;
      /usr/bin/mc mb -p minio/${S3_BUCKET} || true ;
      /usr/bin/mc policy set none minio/${S3_BUCKET} ;
      echo 'MinIO bucket ready: ${S3_BUCKET}'
      "

  # ----------------------------------------------------------
  # Postgres
  # ----------------------------------------------------------
  postgres:
    image: postgres:13
    hostname: postgres
    container_name: postgres
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
    volumes:
      - postgres_db_volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ----------------------------------------------------------
  # Redis
  # ----------------------------------------------------------
  redis:
    image: redis:7
    hostname: redis
    container_name: redis
    expose:
      - "6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s

  # ----------------------------------------------------------
  # Airflow webserver
  # ----------------------------------------------------------
  airflow-webserver:
    image: airflow-custom:2.6.0-spark
    build:
      context: .
      dockerfile: Dockerfile.airflow-worker
    hostname: airflow-webserver
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: "CeleryExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
      AIRFLOW__CELERY__RESULT_BACKEND: "db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
      AIRFLOW__CELERY__BROKER_URL: "${REDIS_URL}"
      AIRFLOW__CORE__FERNET_KEY: "${AIRFLOW__CORE__FERNET_KEY}"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_CONN_SPARK_DEFAULT: "${SPARK_MASTER_URL}"
    env_file: .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark/jars:/opt/extra-jars:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - ./airflow/logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ----------------------------------------------------------
  # Airflow scheduler
  # ----------------------------------------------------------
  airflow-scheduler:
    image: airflow-custom:2.6.0-spark
    hostname: airflow-scheduler
    container_name: airflow-scheduler
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: "CeleryExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
      AIRFLOW__CELERY__RESULT_BACKEND: "db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
      AIRFLOW__CELERY__BROKER_URL: "${REDIS_URL}"
      AIRFLOW__CORE__FERNET_KEY: "${AIRFLOW__CORE__FERNET_KEY}"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_CONN_SPARK_DEFAULT: "${SPARK_MASTER_URL}"
    env_file: .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark/jars:/opt/extra-jars:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - ./airflow/logs:/opt/airflow/logs
    command: scheduler

  # ----------------------------------------------------------
  # Airflow worker
  # ----------------------------------------------------------
  airflow-worker:
    image: airflow-custom:2.6.0-spark
    hostname: airflow-worker
    container_name: airflow-worker
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: "CeleryExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
      AIRFLOW__CELERY__RESULT_BACKEND: "db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
      AIRFLOW__CELERY__BROKER_URL: "${REDIS_URL}"
      AIRFLOW__CORE__FERNET_KEY: "${AIRFLOW__CORE__FERNET_KEY}"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_CONN_SPARK_DEFAULT: "${SPARK_MASTER_URL}"
    env_file: .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark/apps:/opt/spark/apps:ro
      - ./spark/jars:/opt/extra-jars:ro
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - ./airflow/logs:/opt/airflow/logs
    command: celery worker

  # ----------------------------------------------------------
  # Airflow one-time init
  # ----------------------------------------------------------
  airflow-init:
    image: apache/airflow:2.6.0
    hostname: airflow-init
    container_name: airflow-init
    profiles: ["init"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: "CeleryExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
      AIRFLOW__CELERY__RESULT_BACKEND: "db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
      AIRFLOW__CELERY__BROKER_URL: "${REDIS_URL}"
      AIRFLOW__CORE__FERNET_KEY: "${AIRFLOW__CORE__FERNET_KEY}"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      _AIRFLOW_DB_UPGRADE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: "${AIRFLOW_ADMIN_USER}"
      _AIRFLOW_WWW_USER_PASSWORD: "${AIRFLOW_ADMIN_PASSWORD}"
      AIRFLOW_CONN_SPARK_DEFAULT: "${SPARK_MASTER_URL}"
    env_file: .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    command: version

volumes:
  postgres_db_volume:
  airflow_logs:
